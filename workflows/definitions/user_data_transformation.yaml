# User Data Transformation Workflow
# 
# This workflow:
# 1. Reads user data from a JSON file
# 2. Transforms the data using JSONPath expressions
# 3. Logs the process
# 4. Writes the transformed data to a new file

id: user_data_transformation
name: User Data Transformation Workflow
description: Reads user data, transforms it, and writes it to a new file
version: "1.0"
created_at: "2025-05-14T12:00:00Z"
tags:
  - example
  - data-transformation
  - file-processing

# Workflow-level configuration
config:
  log_level: info
  max_retries: 3
  retry_delay: 5
  timeout: 60

# Node definitions
nodes:
  # File Reader Node
  read_users:
    type: FileReader
    label: Read Users JSON File
    description: Reads user data from JSON file
    config:
      file_path: ./data/users.json
      format: json
      encoding: utf-8
    retry:
      max_attempts: 2
      backoff_factor: 1.5

  # JSON Transformer Node
  transform_users:
    type: JSONTransformer
    label: Transform User Data
    description: Extracts and transforms user data
    config:
      transformations:
        - source_path: "$.users[*]"
          result_key: "user_list"
          flatten_array: true
        - source_path: "$.user_list[*].subscription.plan"
          result_key: "plans"
          flatten_array: true
          unique_values: true
        - source_path: "$.user_list[*].tags[*]"
          result_key: "all_tags"
          flatten_array: true
          unique_values: true
        - source_path: "$.user_list[*]"
          result_key: "transformed_users"
          flatten_array: true
          mapping:
            user_id: "$.id"
            full_name: "$.name"
            contact: "$.email"
            location: "$.address.city + ", " + $.address.state"
            is_customer: "$.tags[*] ? contains(@, "customer")"
            subscription_type: "$.subscription.plan"
            is_active: "$.subscription.active"

  # Logger Node
  workflow_logger:
    type: Logger
    label: Workflow Logger
    description: Logs workflow execution steps
    config:
      level: info
      outputs:
        - type: console
        - type: file
          config:
            file_path: ./data/workflow.log
      format: json
      include_context: true
      max_memory_logs: 1000

  # File Writer Node
  write_transformed:
    type: FileWriter
    label: Write Transformed Data
    description: Writes transformed data to a new JSON file
    config:
      file_path: ./data/transformed_users.json
      format: json
      encoding: utf-8
      create_dir: true
      json_options:
        indent: 2
        sort_keys: true

# Connections between nodes
connections:
  # File reader to transformer
  - source: read_users
    target: transform_users
    source_output: data # This should match the output key from FileReaderNode
    target_input: data  # This should match an expected input key for JSONTransformerNode
  
  # File reader to logger (example of logging file path)
  # This connection might need adjustment based on how LoggerNode takes dynamic messages
  - source: read_users
    target: workflow_logger
    source_output: file_path # Output from FileReaderNode
    target_input: message_data # Custom input key for LoggerNode to format message
    # The LoggerNode expects inputs like {"message": "...", "data": {...}, "level": "..."}
    # This connection implies the LoggerNode needs to be able to take `message_data` 
    # and use the `metadata` to format a log message. This is not standard in the current LoggerNode.
    # A more typical pattern would be a dedicated node or a script within a generic script node
    # to format the log message and then pass it to the LoggerNode.
    # For now, I will assume the workflow engine or a pre-processing step handles this, 
    # or the LoggerNode needs to be enhanced. The current LoggerNode takes a direct `message` input.
    # Let's simplify this for now and assume the logger is used to log specific messages triggered by other nodes' completion or data.
    # The current workflow engine's `_prepare_node_input` might handle this mapping if `target_input` is a key in the node's input schema.
    # The LoggerNode's `execute` method expects `inputs["message"]`. 
    # The connection `target_input: data` for `transform_users -> workflow_logger` is also problematic for the same reason.
    # I will comment out these problematic logger connections for the initial test, as they require more advanced data mapping or node capabilities.

  # Transformer to file writer
  - source: transform_users
    target: write_transformed
    source_output: result # This should be the key containing the transformed data from JSONTransformerNode
    target_input: data    # This is the expected input key for FileWriterNode

# Error handling
error_handling:
  default_strategy: stop
  node_strategies:
    read_users: retry
    transform_users: continue
  
  on_failure:
    notify:
      - type: email
        to: "admin@example.com"
      - type: webhook
        url: "https://hooks.example.com/workflow-alerts"

# Scheduling
schedule:
  enabled: true
  cron: "0 0 * * *"  # Run daily at midnight
  timezone: "UTC"

# Monitoring
monitoring:
  metrics:
    - name: execution_time
      description: Total workflow execution time
    - name: records_processed
      description: Number of records processed
  
  alerts:
    - condition: "execution_time > 60"
      severity: warning
      message: "Workflow execution taking longer than expected"

